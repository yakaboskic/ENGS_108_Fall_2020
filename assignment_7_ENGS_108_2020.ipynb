{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_7_ENGS_108_2020",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIiHyjwxYvwM7VLftO3ZTc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakaboskic/ENGS_108_Fall_2020/blob/master/assignment_7_ENGS_108_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R395Ek0FH-Dj"
      },
      "source": [
        "# **ENGS 108 Fall 2020 Assignment 7**\n",
        "\n",
        "*Due November 18, 2020 at 11:59PM on Canvas*\n",
        "\n",
        "**Instructors:** George Cybenko\n",
        "\n",
        "**TAs:** Chase Yakaboski\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Rules and Requirements**\n",
        "\n",
        "\n",
        "1.   You are only allowed to use Python packages that are explicity imported in \n",
        "the assignment notebook or are standard (bultin) python libraries like random, os, sys, etc, (Standard Bultin Python libraries will have a Python.org documentation). For this assignment you may use:\n",
        "  *   [numpy](https://numpy.org/doc/stable/)\n",
        "  *   [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "  *   [scikit-learn](https://scikit-learn.org/stable/)\n",
        "  *   [matplotlib](https://matplotlib.org/)\n",
        "  *   [tensorflow](https://www.tensorflow.org/)\n",
        "\n",
        "2.   All code must be fit into the designated code or text blocks in the assignment notebook. They are indentified by a **TODO** qualifier.\n",
        "\n",
        "3. For analytical questions that don't require code, type your answer cleanly in Markdown. For help, see the [Google Colab Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qCEgj_4ICNV"
      },
      "source": [
        "'''Import Statements'''\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0otRQ4_CcxP4"
      },
      "source": [
        "## **Problem 1: MNIST GANs**\n",
        "Use the MNIST dataset to train 10 Generative Adversarial Networks to create hand writtencharacters using the MNIST samples. You can build on the Matlab example or use any othercode as a starting point. Submit your code for the GAN you used.\n",
        "\n",
        "> **Part 1** Load in the MNIST dataset and seperate the dataset into 10 seperate datasets for each number class, i.e. 0, 1, 2, ..., 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4fYMGxGhrna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024dea4b-5162-4b70-d3e8-d4848a379995"
      },
      "source": [
        "# Load data\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "#TODO: Seperate the training data using the train labels into 10 seperate datasets.\n",
        "\n",
        "#TODO: Then build a TF dataset for each of the seperated out numpy datasets."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1dkkXbnMJuh"
      },
      "source": [
        "> **Part 2** Make the generator and the discriminator of the GAN. Feel free to use [this tutorial](https://www.tensorflow.org/tutorials/generative/dcgan) for building the generator and discriminator. Maybe change the architecture a little bit tho so it's not just straight plagiarism.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPI9wZwLJlBj"
      },
      "source": [
        "#TODO: Build a function that builds the generator network.\n",
        "def make_generator_model():\n",
        "  return model\n",
        "#TODO: Build a function that builds the discriminator network.\n",
        "def make_discriminator_model():\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwQGD_7dNRzK"
      },
      "source": [
        "> **Part 3** Define your loss functions for the models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNZwbcUsNyGX"
      },
      "source": [
        "#TODO: Make your generator and discriminiator loss functions and assign a optimizer for your generator and discriminator.\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  return loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvVAePxmOhSE"
      },
      "source": [
        "> **Part 4** Using the training step function provided build a training loop and train 10 GANs using your 10 datasets so that each GAN is trained on a seperate dataset. After training, you should have 10 GANs, where each GAN will generate one respective number, i.e. 0, 1, 2, ..., 9. **Make sure to enable GPU acceleration for faster training. Go to *Edit > Notebook Settings > Hardware Accelerator* and click GPU.** *Note: You'll have to rerun your notebook after you do this.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1evkmy8PTGzm"
      },
      "source": [
        "# Some code to help save checkpoints in case your notebook crashes\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfj_K5kBOcKY"
      },
      "source": [
        "noise_dim = 100\n",
        "\n",
        "# Provided training step function taken from cited tutorial but modified a little. Pay attention to the modification!\n",
        "def train_step_fn():\n",
        "  @tf.function\n",
        "  def train_step(images, generator, discriminator, generator_optimizer, discriminator_optimizer):\n",
        "      noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "  return train_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDAO2_j3O_u4"
      },
      "source": [
        "#TODO: Finish implementing this train function. It doesn't have to return anything\n",
        "def train(dataset, epochs, generator, discriminator, generator_optimizer, discriminator_optimizer, checkpoint):\n",
        "  for epoch in tq.tqdm(range(epochs), desc='Completed Epochs'):\n",
        "    start = time.time()\n",
        "    \n",
        "    #TODO: Implement training step\n",
        "    \n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE2Bw36kPUAp"
      },
      "source": [
        "#TODO: Finish the training loop.\n",
        "EPOCHS = 50\n",
        "gans = []\n",
        "\n",
        "for dataset in tf_datasets:\n",
        "  #TODO: Define your optimizers, i.e. change the Nones to something.\n",
        "  generator_optimizer = None\n",
        "  discriminator_optimizer = None\n",
        "  # Instaniate your models\n",
        "  gen = make_generator_model()\n",
        "  discrim = make_discriminator_model()\n",
        "  # Reinitialize the train_step function\n",
        "  train_step = train_step_fn()\n",
        "  # Put in your checkoutpoint for each gan\n",
        "  checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=gen,\n",
        "                                 discriminator=discrim)\n",
        "  # Train the model\n",
        "  train(dataset, EPOCHS, gen, discrim, generator_optimizer, discriminator_optimizer, checkpoint)\n",
        "  # Save the generator in the gans list\n",
        "  gans.append(gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0lruBvBan8o"
      },
      "source": [
        "> **Part 5** Using a random seed as the generator input, plot the generated number from each of the ten GANs. Use the same seed for each GAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFfzIbnGa--v"
      },
      "source": [
        "#TODO: Initialize a random seed as input into each gan.\n",
        "\n",
        "#TODO: Go through each GAN, output the result using the seed and plot the result."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_Kr26T6dx3W"
      },
      "source": [
        "## **Problem 2: MNIST RCO**\n",
        "Use the 10 trained generators to create the following application which we will call RCO(reverse of OCR).\n",
        "* Your RCO will read a string of digits and will output the corresponding GAN generated samples of handwritten digits as a single png image of the string of digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMTCiRXGb8aY"
      },
      "source": [
        "#TODO: Implement the following function\n",
        "def rco(string_of_numbers):\n",
        "  generated_numbers = []\n",
        "  for num in string_of_numbers:\n",
        "    #TODO: Convert the number to an int\n",
        "    #TODO: Index your trained GANs list with that int\n",
        "    #TODO: Generate a image using your seed from Part 5. \n",
        "    #TODO: Append that img to generated_numbers list\n",
        "\n",
        "  #TODO: Now go through your generated numbers a create a matplotlib plot of all the numbers in the string.\n",
        "  # Maybe increase figure size if too small.\n",
        "  fig = plt.figure(15, 15)\n",
        "  for i, gen_num in enumerate(generated_numbers):\n",
        "    plt.subplot(1, len(generated_numbers), i+1)\n",
        "    #TODO: Implement the image show.\n",
        "    plt.imshow()\n",
        "    #TODO: Maybe turn off the axis\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}