{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assign_6_ENGS_108_Fall_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1EXaYP-OTNO9eltRbwKEkUondRH_HxZgx",
      "authorship_tag": "ABX9TyO6sjRlbgKRD0mt9eBA9aQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakaboskic/ENGS_108_Fall_2020/blob/master/assign_5_ENGS_108_Fall_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BiHNk7HcRiL"
      },
      "source": [
        "# **ENGS 108 Fall 2020 Assignment 5**\n",
        "\n",
        "*Due November 9, 2020 at 11:59PM on Canvas*\n",
        "\n",
        "**Instructors:** George Cybenko\n",
        "\n",
        "**TAs:** Chase Yakaboski\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Rules and Requirements**\n",
        "\n",
        "\n",
        "1.   You are only allowed to use Python packages that are explicity imported in \n",
        "the assignment notebook or are standard (bultin) python libraries like random, os, sys, etc, (Standard Bultin Python libraries will have a Python.org documentation). For this assignment you may use:\n",
        "  *   [numpy](https://numpy.org/doc/stable/)\n",
        "  *   [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "  *   [scikit-learn](https://scikit-learn.org/stable/)\n",
        "  *   [matplotlib](https://matplotlib.org/)\n",
        "  *   [tensorflow](https://www.tensorflow.org/)\n",
        "\n",
        "2.   All code must be fit into the designated code or text blocks in the assignment notebook. They are indentified by a **TODO** qualifier.\n",
        "\n",
        "3. For analytical questions that don't require code, type your answer cleanly in Markdown. For help, see the [Google Colab Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KqjPtWlcFnS"
      },
      "source": [
        "''' Import Statements '''\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "import itertools\n",
        "\n",
        "import random\n",
        "# Don't mess with this (gives reproducible results)\n",
        "random.seed(444)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0otRQ4_CcxP4"
      },
      "source": [
        "## **Problem 1: Reinforcement Learning**\n",
        "In this problem we will play a game of cops and robbers. The game is played on a fixed undirected, simple, and finite graph $G$. There are two players, a cop and a robber. It is the goal of the cop to catch the robber in as few moves as possible. \n",
        "\n",
        "The graph $G$ has the following properties:\n",
        "  - A total of $m$ nodes.\n",
        "  - It contains a single $n$ node cycle, where $n\\leq m$, and random additional edges to make the graph connected.\n",
        "\n",
        "The game starts, with the cop taking their choice of vertex in $G$ and then the robber selects a random vertex in $G$ that is not occupied by the cop. At every point in the game both players know the positions of each other, and in this version of the problem we will say that the robber is drunk (i.e. they will randomly choose there next that instead of employing a policy).\n",
        "\n",
        "The availabe actions of the cop and associated reward function is:\n",
        "  - Move to a node not connected to their present node (and the cop stays in the current position): -5.\n",
        "  - Move to an adjacent node (including staying at current node): -1.\n",
        "  - Move to the node occupied by robber: +100.\n",
        ">\n",
        "> **Part 1** Building a Graph Class.\n",
        ">> **(a)** Using the provided skeleton build a general graph class for this problem for $n$ nodes. You are expected to implement *add_edge*, *make_random_graph*, *check_connected*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaHjFProBqKc"
      },
      "source": [
        "class Graph:\n",
        "  \"\"\" Our graph class.\n",
        "  Args:\n",
        "    n: Number of nodes in our cycle.\n",
        "    m: Total number of nodes in graph, where n >= m.\n",
        "  \"\"\"\n",
        "  def __init__(self, n, m):\n",
        "    self.n = n\n",
        "    self.m = m\n",
        "    # A default dict is just a dict that won't raise a KeyError, it instead fills\n",
        "    # the unknown key with a default, in our case an empty list.\n",
        "    self.G = defaultdict(list)\n",
        "    \n",
        "  def add_edge(self, u, v):\n",
        "    \"\"\"\n",
        "    Make a function that will add an edge to the graph.\n",
        "    \"\"\"\n",
        "    #TODO: Implement.\n",
        "\n",
        "\n",
        "  def make_random_graph(self):\n",
        "    \"\"\" First make a cycle of given length and then add random additional edges\n",
        "    in such a way that the final graph will be connected.\n",
        "    \"\"\"\n",
        "    #TODO: Make n length cycle first\n",
        "    \n",
        "    #TODO: Add additional nodes until random graph is connected\n",
        "    while True:\n",
        "      _graph = copy.copy(self)\n",
        "      #TODO: Use the graph copy to add random nodes/edges\n",
        "      #TODO: Check if the new graph is connected.\n",
        "      if self.check_connected(_graph.G):\n",
        "        # If it is set the current graph's adjacency matrix equal to the copy's.\n",
        "        self.graph = _graph.G\n",
        "        # Return (i.e. break the loop)\n",
        "        return True\n",
        "\n",
        "  def check_connected(self, G):\n",
        "    \"\"\" Perform a Depth First Search.\n",
        "    \"\"\"\n",
        "    start_node = random.choice(list(G.keys()))\n",
        "    return dfs(G, {}, start_node, start_node)\n",
        "\n",
        "def dfs(G, visited, u, parent):\n",
        "  \"\"\" Implement your own depth first search. Many online resources for this.\n",
        "  Args:\n",
        "    G: Is the graph (adjency matrix) we want to test.\n",
        "    visited: Is a dictionary that keeps track of the nodes you've visited.\n",
        "    u: a starting node.\n",
        "    parent: The parent node for DFS so that you can cancel recursion if you complete the cycle.\n",
        "  Returns:\n",
        "    True: If all nodes have been visited at least once.\n",
        "    False: Otherwise.\n",
        "  \"\"\"\n",
        "  #TODO: Implement. Hint: Make sure to keep track of the parent node for a DFS search,\n",
        "  # as a cycle leads to an infinite recursion, so you have to make sure you break the cycle."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc44XK7_JEYO"
      },
      "source": [
        ">> **(b)** Test out your graph class with a cops and robbers graph of $n=5$ and $m=10$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKxMuq_7BkGP"
      },
      "source": [
        "#TODO: Your code goes here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTuchWzJJaxk"
      },
      "source": [
        "> **Part 2** Understanding the state space, i.e. the Game.\n",
        ">> **(a)** Given the graph class you've created in Part 1. Develop a Cops and Robbers game class. Use the skeleton below to implement the following functions first: *get_successors*, *terminal_test*, *result*. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qBoFrNDBT9y"
      },
      "source": [
        "class CopsAndRobbers:\n",
        "  def __init__(self, graph, start_state, rewards_table=None):\n",
        "    \"\"\" This is the cops and robbers game class.\n",
        "    Args:\n",
        "      start_state: The starting state of the cop position and robber position in\n",
        "        the graph. Should be a tuple of form (cop_pos, rob_pos).\n",
        "      G: The graph adjacency matrix\n",
        "      state: current state in the game.\n",
        "      reward_table: The (state, actions) reward dictionary that you will eventually implement.\n",
        "    \"\"\"\n",
        "    self.graph = graph\n",
        "    self.G = graph.G\n",
        "    self.state = start_state\n",
        "    self.rewards_table = rewards_table\n",
        "\n",
        "  def terminal_test(self, state):\n",
        "    #TODO: Implement.\n",
        "\n",
        "  def get_successors(self, state):\n",
        "    \"\"\" Return a list of successor states that can be reached from the current state.\n",
        "    Hint: Remember only the cop can choose their action.\n",
        "    \"\"\"\n",
        "    #TODO: Implement.\n",
        "\n",
        "  def result(self, state, next_state):\n",
        "    \"\"\" This function should return the state after the cop has made their move,\n",
        "    and the drunk robber has moved accordingly.\n",
        "    Args:\n",
        "      state: Current state of (cop, rob).\n",
        "      next_state: The state after the cop has moved (next_cop, rob). Calculated from get_successors.\n",
        "    \"\"\"\n",
        "    # TODO: Implement.\n",
        "\n",
        "  def utility(self, state, action):\n",
        "    return self.rewards_table[(state, action)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-YjCqNsXOKG"
      },
      "source": [
        ">> **(b)** In reinforcement learning we are often interested in calculating a rewards table that has possible states as its rows and possible actions as its columns and filled in with the associated reward given the Q(state, action) pair. Calculate the rewards table for any given graph. *Hint: This should be an $m^2$ x $m$ matrix or a dictionary with $m^3$ keys such that the keys are (state, action) tuples.*  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F61FjlM__wEg"
      },
      "source": [
        "def calculateRewardsTable(graph):\n",
        "  \"\"\" Make a rewards table dictionary of the from table[(state, action)] = reward.\n",
        "  \"\"\"\n",
        "  #TODO: implement\n",
        "  return table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU0Hq2C8bocS"
      },
      "source": [
        ">> **(c)** Now that we have our reward table, try to solve the problem in a brute-force manner (without reinforcement learning). I.e. try to reach the terminal state, or find get a reward of 100. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GysY6hjZ_Iu5"
      },
      "source": [
        "epochs = 0\n",
        "rewards = []\n",
        "penalties = 0\n",
        "#TODO: Instantiate your graph\n",
        "#TODO: Build your rewards table\n",
        "#TODO: Instantiate your game.\n",
        "\n",
        "# Simulation loop\n",
        "while True:\n",
        "  #TODO: Get the next state, check for terminal state, and get reward\n",
        "  # Make sure to break if you reach the terminal state."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec8eFd77jdiV"
      },
      "source": [
        "> **Part 3** Q-learning. \n",
        ">\n",
        "> Up to this point we have build a graph class, built a game class, ran a brute-force simulation of an agent traversing the space randomly, and now we will dive into Q-learning in the hopes of maximizing the rewards and efficiency of capturing the drunk robber. \n",
        ">\n",
        ">> **(a)** Using the skeleton from the brute force method, implement a training loop to learn a Q-table for a given graph and game. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhpxYwKn-rIa"
      },
      "source": [
        "#TODO: Your code goes here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FVu7FlNk3IA"
      },
      "source": [
        ">> **(b)** Evalute your new Q-learning agent over a 100 epochs, by choosing your actions based on the argmax of the Q-table caluculated in (a) and report the average number of penalities, average time, and average number of steps it took to find the robber with your new Q-learning strategy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqnDZQ3C-1qE"
      },
      "source": [
        "#TODO: Your code goes here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrpB9jKolvH_"
      },
      "source": [
        ">> **(c)** Compare your results with the brute-force method used in Part 2 and comment on the improvement. For instance, try varying graph configurations and look for any signs of improvement in certian instances. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6b7G2mB_FIr"
      },
      "source": [
        "#TODO: Your code goes here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDZuSjRS944s"
      },
      "source": [
        "> **Bonus** Check that the learned policy satsifies the [Bellman Inequality](https://towardsdatascience.com/mathematical-analysis-of-reinforcement-learning-bellman-equation-ac9f0954e19f), i.e is the computed solution the actual optimal policy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BKvya8C_GJH"
      },
      "source": [
        "#TODO: Your code goes here."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
